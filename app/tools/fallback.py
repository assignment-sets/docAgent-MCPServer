from app.dependencies import get_gemini_llm
from app.schemas import FallbackInput, PyRuntimeInput
from app.tools import exec_py_runtime

from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.runnables import Runnable

import logging

logger = logging.getLogger(__name__)


async def fallback_tool(input: FallbackInput) -> str:
    try:
        logger.info("[üß†] Fallback tool invoked with prompt:\n%s", input.prompt)

        llm = get_gemini_llm()

        system_prompt = SystemMessage(
            content="""
You are a Python coding assistant. Your only job is to generate valid Python code to accomplish the user's task using ONLY Python. you will recieve input task in natural language.

Environment Details:
- Your code will be executed inside a sandboxed python runtime which has network access.
- A file system watcher is running in the container that automatically uploads any files you create to cloud storage and returns public URLs for them.
- The watcher monitors the current working directory and uploads any **new files**, returning a temporary URL for each.
- DO NOT attempt to upload files yourself or generate URLs ‚Äî that is already automated and handled by the system.
- you currently have only these dependencies installed in the env numpy, pandas, matplotlib, seaborn, scipy, sympy, scikit-learn, requests

Constraints:
- You must assume that your code will be executed in a sandboxed python runtime with no GUI.
- DO NOT print results to stdout.
- Write results to files in the **current working directory** (do not use subdirectories).
- Generate python code that solves the task.
- IF YOU CAN NOT SOLVE GIVEN TASK WITH THE GIVEN ENVIRONMENT AND CONSTRAINTS JUST PRINT THE REASON USING PRINT FUNCTION AND DO NOT WRITE FURTHER CODE OR SAVE ANY FILE AS THAT WILL WASTE TIME.
""".strip()
        )

        # Construct the full message sequence
        messages = [system_prompt, HumanMessage(content=input.prompt)]

        # Bind the LLM to output PyRuntimeInput format
        structured_llm: Runnable = llm.with_structured_output(PyRuntimeInput)

        # Call the LLM
        output: PyRuntimeInput = await structured_llm.ainvoke(messages)
        # print(output)

        logger.info("[‚úÖ] Code successfully generated by fallback LLM.")
        logger.info(output)

        # Execute the generated Python code using the py_runtime tool
        result = await exec_py_runtime(output)

        return result

    except Exception as e:
        logger.exception("[‚ùå] Fallback tool execution failed.")
        return f"Fallback failed: {str(e)}"


if __name__ == "__main__":
    import asyncio

    ip = FallbackInput(
        prompt="Can you please put the text \'\\u09b9\\u09cd\\u09af\\u09be\\u09b2\\u09cb, \\u0986\\u09ae\\u09bf \\u098f\\u0995\\u09be\\u0987 \\u09b8\\u09ae\\u09cd\\u09ae\\u09be\\u09a8\\u09bf\\u09a4\\u0964\' into a text file? "
    )

    async def test_fallback():
        op = await fallback_tool(ip)
        print(f"op: {op}")

    asyncio.run(test_fallback())
